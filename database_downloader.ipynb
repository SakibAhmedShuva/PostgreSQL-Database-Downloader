{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import csv\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective column export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db_config() -> dict:\n",
    "    \"\"\"Load database configuration from environment variables.\"\"\"\n",
    "    load_dotenv()\n",
    "    return {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USERNAME'),\n",
    "        'password': os.getenv('DB_PASS'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "        'port': os.getenv('DB_PORT')\n",
    "    }\n",
    "\n",
    "def export_to_csv(columns: List[str], output_file: str = 'user_export.csv') -> None:\n",
    "    \"\"\"\n",
    "    Export specified columns from the user table to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        columns: List of column names to export\n",
    "        output_file: Name of the output CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        db_config = load_db_config()\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create the SQL query with schema.table name\n",
    "        columns_str = ', '.join(columns)\n",
    "        query = f'SELECT {columns_str} FROM public.user'\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Write to CSV\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Write header\n",
    "            writer.writerow(columns)\n",
    "            # Write data\n",
    "            for row in cursor:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Data successfully exported to {output_file}\")\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to user_export.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    columns_to_export = [\"id\", \"email\", \"password\", \"role\"]\n",
    "    export_to_csv(columns_to_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full table export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full table successfully exported to user_table_export_20241230_220628.csv\n",
      "Total columns exported: 28\n",
      "Columns: id, email, password, role, isConfirmed, isActive, fullName, profilePicture, countryId, phone, additionalPhone, address01, address02, dateOfBirth, gender, createdAt, updatedAt, brandId, verificationToken, userId, isDeleted, isUserRegisterForBrand, passWordChangeRequest, isOwner, isBiometric, newPassword, contactPersonId, isBlocked\n"
     ]
    }
   ],
   "source": [
    "def load_db_config() -> dict:\n",
    "    \"\"\"Load database configuration from environment variables.\"\"\"\n",
    "    load_dotenv()\n",
    "    return {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USERNAME'),\n",
    "        'password': os.getenv('DB_PASS'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "        'port': os.getenv('DB_PORT')\n",
    "    }\n",
    "\n",
    "def export_full_table_to_csv():\n",
    "    \"\"\"Export the entire user table to a CSV file with timestamp in filename.\"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        db_config = load_db_config()\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get column names\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name = 'user'\n",
    "            ORDER BY ordinal_position;\n",
    "        \"\"\")\n",
    "        columns = [col[0] for col in cursor.fetchall()]\n",
    "        \n",
    "        # Create filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f'user_table_export_{timestamp}.csv'\n",
    "        \n",
    "        # Execute the query for all data\n",
    "        cursor.execute('SELECT * FROM public.user')\n",
    "        \n",
    "        # Write to CSV\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Write header\n",
    "            writer.writerow(columns)\n",
    "            # Write data\n",
    "            for row in cursor:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Full table successfully exported to {filename}\")\n",
    "        print(f\"Total columns exported: {len(columns)}\")\n",
    "        print(f\"Columns: {', '.join(columns)}\")\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_full_table_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Database Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an export method:\n",
      "1. Full database backup using pg_dump (recommended for backups)\n",
      "2. Export all tables to individual CSV files (data only)\n",
      "3. Export a single table to CSV\n",
      "Exporting all tables to directory: ninja-dev_all_tables_csv_20250517_230044\n",
      "\n",
      "Exporting table: _prisma_migrations...\n",
      "Table '_prisma_migrations' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\_prisma_migrations_export_20250517_230045.csv\n",
      "\n",
      "Exporting table: task...\n",
      "Table 'task' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\task_export_20250517_230045.csv\n",
      "\n",
      "Exporting table: otp...\n",
      "Table 'otp' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\otp_export_20250517_230046.csv\n",
      "\n",
      "Exporting table: purchase_plan...\n",
      "Table 'purchase_plan' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\purchase_plan_export_20250517_230046.csv\n",
      "\n",
      "Exporting table: newLetter...\n",
      "Table 'newLetter' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\newLetter_export_20250517_230047.csv\n",
      "\n",
      "Exporting table: interestUser...\n",
      "Table 'interestUser' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\interestUser_export_20250517_230047.csv\n",
      "\n",
      "--- Export Summary ---\n",
      "Successfully exported tables: 6\n",
      "Failed to export tables: 0\n",
      "All tables exported successfully to CSVs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_db_config() -> dict:\n",
    "    \"\"\"Load database configuration from environment variables.\"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    db_port_str = os.getenv('DB_PORT')\n",
    "    db_port = None\n",
    "    if db_port_str and db_port_str.isdigit():\n",
    "        db_port = int(db_port_str)\n",
    "    elif db_port_str:\n",
    "        print(f\"Warning: DB_PORT ('{db_port_str}') is not a valid number. Using default port if applicable.\")\n",
    "\n",
    "    config = {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USERNAME'),\n",
    "        'password': os.getenv('DB_PASS'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "    }\n",
    "    if db_port:\n",
    "        config['port'] = db_port\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        if value is None and key not in ['port', 'password']: # Password can be handled by PGPASSWORD or .pgpass\n",
    "             print(f\"Warning: Environment variable for '{key.upper()}' (e.g., DB_{key.upper()}) is not set.\")\n",
    "             \n",
    "    return config\n",
    "\n",
    "def export_entire_database_pg_dump(output_format: str = 'custom'):\n",
    "    \"\"\"\n",
    "    Export the entire database using pg_dump.\n",
    "    Requires pg_dump to be in the system's PATH or provide full path.\n",
    "    \n",
    "    Args:\n",
    "        output_format (str): 'custom' (for .dump, compressed, pg_restore), \n",
    "                             'plain' (for .sql, text SQL commands),\n",
    "                             'directory' (for parallel dumps).\n",
    "                             Defaults to 'custom'.\n",
    "    \"\"\"\n",
    "    db_config = load_db_config()\n",
    "\n",
    "    if not db_config.get('dbname'):\n",
    "        print(\"Error: DB_NAME is not configured. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Determine file extension and pg_dump format option\n",
    "    if output_format == 'custom':\n",
    "        file_extension = 'dump'\n",
    "        format_option = '-Fc' # Custom format (compressed, good for pg_restore)\n",
    "    elif output_format == 'plain':\n",
    "        file_extension = 'sql'\n",
    "        format_option = '-Fp' # Plain SQL text\n",
    "    elif output_format == 'directory':\n",
    "        # For directory format, pg_dump expects a directory name, not a file\n",
    "        # We'll create a directory and pg_dump will populate it.\n",
    "        # The filename variable will represent the directory name.\n",
    "        file_extension = '' # No extension for directory name\n",
    "        format_option = '-Fd' # Directory format\n",
    "        output_dir_name = f\"{db_config['dbname']}_fulldb_export_{timestamp}\"\n",
    "        filename = output_dir_name # This is now a directory\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Error: Output directory '{filename}' already exists.\")\n",
    "            return\n",
    "        # No need to create the directory beforehand for -Fd, pg_dump does it.\n",
    "    else:\n",
    "        print(f\"Error: Unsupported output format '{output_format}'. Choose 'custom', 'plain', or 'directory'.\")\n",
    "        return\n",
    "\n",
    "    if output_format != 'directory':\n",
    "        filename = f\"{db_config['dbname']}_fulldb_export_{timestamp}.{file_extension}\"\n",
    "\n",
    "\n",
    "    pg_dump_command = ['pg_dump']\n",
    "    \n",
    "    # Add connection parameters\n",
    "    if db_config.get('host'):\n",
    "        pg_dump_command.extend(['-h', db_config['host']])\n",
    "    if db_config.get('port'):\n",
    "        pg_dump_command.extend(['-p', str(db_config['port'])])\n",
    "    if db_config.get('user'):\n",
    "        pg_dump_command.extend(['-U', db_config['user']])\n",
    "    \n",
    "    pg_dump_command.append(format_option)\n",
    "    pg_dump_command.append(db_config['dbname'])\n",
    "\n",
    "    # Environment variables for pg_dump\n",
    "    env = os.environ.copy()\n",
    "    if db_config.get('password'):\n",
    "        env['PGPASSWORD'] = db_config['password']\n",
    "        # Note: It's generally more secure to use a .pgpass file for pg_dump\n",
    "        # or rely on other authentication methods (like peer authentication if applicable).\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting database dump for '{db_config['dbname']}'...\")\n",
    "        print(f\"Command (password hidden): {' '.join(pg_dump_command)} > {filename if output_format != 'directory' else output_dir_name}\")\n",
    "\n",
    "        if output_format == 'directory':\n",
    "            pg_dump_command.extend(['-f', filename]) # -f specifies output directory for -Fd\n",
    "            process = subprocess.Popen(pg_dump_command, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout, stderr = process.communicate()\n",
    "        else: # custom or plain\n",
    "            with open(filename, 'wb') as f_out: # 'wb' for binary formats like custom\n",
    "                process = subprocess.Popen(pg_dump_command, stdout=f_out, stderr=subprocess.PIPE, env=env)\n",
    "                stdout, stderr = process.communicate() # stdout is redirected to file, capture stderr\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            print(f\"Database '{db_config['dbname']}' successfully exported to '{filename}'\")\n",
    "            if stderr:\n",
    "                print(f\"pg_dump warnings/messages:\\n{stderr.decode()}\")\n",
    "        else:\n",
    "            print(f\"Error during pg_dump (return code: {process.returncode}):\")\n",
    "            print(f\"STDOUT:\\n{stdout.decode() if stdout else 'N/A'}\")\n",
    "            print(f\"STDERR:\\n{stderr.decode() if stderr else 'N/A'}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'pg_dump' command not found. Make sure PostgreSQL client tools are installed and in your PATH.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- The single table export function from your previous code (can be kept for individual table exports) ---\n",
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "def export_table_to_csv(table_name: str, output_dir: str = \".\"):\n",
    "    \"\"\"Export a single specified table to a CSV file with timestamp in filename.\"\"\"\n",
    "    # (This is the function you provided earlier, slightly adapted for output directory)\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        db_config = load_db_config()\n",
    "        if not db_config.get('dbname') or not db_config.get('user'):\n",
    "            print(\"Error: Database name or user not configured. Please check your .env file.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name = %s\n",
    "            ORDER BY ordinal_position;\n",
    "        \"\"\", (table_name,))\n",
    "        \n",
    "        columns_result = cursor.fetchall()\n",
    "        if not columns_result:\n",
    "            print(f\"Warning: Table '{table_name}' not found in schema 'public', or it has no columns. Skipping.\")\n",
    "            return True # Not an error, just skipping\n",
    "        columns = [col[0] for col in columns_result]\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # Ensure output_dir exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = os.path.join(output_dir, f'{table_name}_export_{timestamp}.csv')\n",
    "        \n",
    "        query_data = sql.SQL(\"SELECT * FROM public.{}\").format(sql.Identifier(table_name))\n",
    "        cursor.execute(query_data)\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            for row in cursor:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Table '{table_name}' successfully exported to {filename}\")\n",
    "        return True # Indicate success\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error for table {table_name}: {e}\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"File system error for table {table_name} (e.g., cannot write file): {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for table {table_name}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if conn: conn.close()\n",
    "\n",
    "# Method 2: Python script to export all tables to individual CSV files (Data Only, No Schema)\n",
    "# This is NOT a full database backup, but can be useful if you just need data in CSVs.\n",
    "\n",
    "def export_all_tables_to_csvs():\n",
    "    \"\"\"Exports all user tables from the 'public' schema to individual CSV files.\"\"\"\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    db_config = load_db_config()\n",
    "    if not db_config.get('dbname'):\n",
    "        print(\"Error: DB_NAME is not configured. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get all table names in the 'public' schema\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT tablename \n",
    "            FROM pg_catalog.pg_tables \n",
    "            WHERE schemaname = 'public';\n",
    "        \"\"\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        if not tables:\n",
    "            print(\"No tables found in the 'public' schema.\")\n",
    "            return\n",
    "\n",
    "        # Create a directory for this export batch\n",
    "        timestamp_dir = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        base_export_dir = f\"{db_config['dbname']}_all_tables_csv_{timestamp_dir}\"\n",
    "        os.makedirs(base_export_dir, exist_ok=True)\n",
    "        print(f\"Exporting all tables to directory: {base_export_dir}\")\n",
    "\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        for table_name in tables:\n",
    "            print(f\"\\nExporting table: {table_name}...\")\n",
    "            if export_table_to_csv(table_name, output_dir=base_export_dir):\n",
    "                success_count += 1\n",
    "            else:\n",
    "                fail_count += 1\n",
    "        \n",
    "        print(f\"\\n--- Export Summary ---\")\n",
    "        print(f\"Successfully exported tables: {success_count}\")\n",
    "        print(f\"Failed to export tables: {fail_count}\")\n",
    "        if fail_count == 0:\n",
    "            print(\"All tables exported successfully to CSVs.\")\n",
    "        else:\n",
    "            print(\"Some tables failed to export. Check logs above.\")\n",
    "\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database connection or initial query error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose an export method:\")\n",
    "    print(\"1. Full database backup using pg_dump (recommended for backups)\")\n",
    "    print(\"2. Export all tables to individual CSV files (data only)\")\n",
    "    print(\"3. Export a single table to CSV\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1, 2, or 3): \").strip()\n",
    "\n",
    "    if choice == '1':\n",
    "        print(\"\\nChoose pg_dump output format:\")\n",
    "        print(\"  c. Custom (.dump - compressed, for pg_restore)\")\n",
    "        print(\"  p. Plain SQL (.sql - text SQL commands)\")\n",
    "        print(\"  d. Directory (outputs to a directory, good for parallel)\")\n",
    "        format_choice_input = input(\"Enter format (c, p, d) [default: c]: \").strip().lower()\n",
    "        \n",
    "        dump_format = 'custom' # default\n",
    "        if format_choice_input == 'p':\n",
    "            dump_format = 'plain'\n",
    "        elif format_choice_input == 'd':\n",
    "            dump_format = 'directory'\n",
    "        elif format_choice_input == 'c' or not format_choice_input:\n",
    "            dump_format = 'custom' # explicit default\n",
    "        else:\n",
    "            print(\"Invalid format choice. Using 'custom'.\")\n",
    "\n",
    "        export_entire_database_pg_dump(output_format=dump_format)\n",
    "    elif choice == '2':\n",
    "        export_all_tables_to_csvs()\n",
    "    elif choice == '3':\n",
    "        target_table = input(\"Enter the name of the single table to export: \").strip()\n",
    "        if not target_table:\n",
    "            print(\"No table name provided. Exiting.\")\n",
    "        else:\n",
    "            if not target_table.replace('_', '').isalnum(): # Basic check\n",
    "                print(f\"Warning: Table name '{target_table}' may contain invalid characters.\")\n",
    "            export_table_to_csv(target_table) # Uses current dir by default\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

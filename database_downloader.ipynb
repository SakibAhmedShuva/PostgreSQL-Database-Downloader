{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import csv\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Column export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db_config() -> dict:\n",
    "    \"\"\"Load database configuration from environment variables.\"\"\"\n",
    "    load_dotenv()\n",
    "    return {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USERNAME'),\n",
    "        'password': os.getenv('DB_PASS'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "        'port': os.getenv('DB_PORT')\n",
    "    }\n",
    "\n",
    "def export_to_csv(columns: List[str], output_file: str = 'user_export.csv') -> None:\n",
    "    \"\"\"\n",
    "    Export specified columns from the user table to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        columns: List of column names to export\n",
    "        output_file: Name of the output CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        db_config = load_db_config()\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create the SQL query with schema.table name\n",
    "        columns_str = ', '.join(columns)\n",
    "        query = f'SELECT {columns_str} FROM public.user'\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Write to CSV\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Write header\n",
    "            writer.writerow(columns)\n",
    "            # Write data\n",
    "            for row in cursor:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Data successfully exported to {output_file}\")\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to user_export.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    columns_to_export = [\"id\", \"email\", \"password\", \"role\"]\n",
    "    export_to_csv(columns_to_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single table export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full table successfully exported to user_table_export_20241230_220628.csv\n",
      "Total columns exported: 28\n",
      "Columns: id, email, password, role, isConfirmed, isActive, fullName, profilePicture, countryId, phone, additionalPhone, address01, address02, dateOfBirth, gender, createdAt, updatedAt, brandId, verificationToken, userId, isDeleted, isUserRegisterForBrand, passWordChangeRequest, isOwner, isBiometric, newPassword, contactPersonId, isBlocked\n"
     ]
    }
   ],
   "source": [
    "def load_db_config() -> dict:\n",
    "    \"\"\"Load database configuration from environment variables.\"\"\"\n",
    "    load_dotenv()\n",
    "    return {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USERNAME'),\n",
    "        'password': os.getenv('DB_PASS'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "        'port': os.getenv('DB_PORT')\n",
    "    }\n",
    "\n",
    "def export_full_table_to_csv():\n",
    "    \"\"\"Export the entire user table to a CSV file with timestamp in filename.\"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        db_config = load_db_config()\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get column names\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name = 'user'\n",
    "            ORDER BY ordinal_position;\n",
    "        \"\"\")\n",
    "        columns = [col[0] for col in cursor.fetchall()]\n",
    "        \n",
    "        # Create filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f'user_table_export_{timestamp}.csv'\n",
    "        \n",
    "        # Execute the query for all data\n",
    "        cursor.execute('SELECT * FROM public.user')\n",
    "        \n",
    "        # Write to CSV\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Write header\n",
    "            writer.writerow(columns)\n",
    "            # Write data\n",
    "            for row in cursor:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Full table successfully exported to {filename}\")\n",
    "        print(f\"Total columns exported: {len(columns)}\")\n",
    "        print(f\"Columns: {', '.join(columns)}\")\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_full_table_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Tables Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2 import sql # For safe SQL identifiers\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any # Added Dict, Any for type hints\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_db_config() -> Dict[str, Any]:\n",
    "    \"\"\"Load database configuration from environment variables.\"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    db_port_str = os.getenv('DB_PORT')\n",
    "    db_port = None\n",
    "    if db_port_str and db_port_str.isdigit():\n",
    "        db_port = int(db_port_str)\n",
    "    elif db_port_str:\n",
    "        print(f\"Warning: DB_PORT ('{db_port_str}') is not a valid number. Using default PostgreSQL port 5432 if applicable by psycopg2.\")\n",
    "\n",
    "    config: Dict[str, Any] = {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USERNAME'),\n",
    "        'password': os.getenv('DB_PASS'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "    }\n",
    "    if db_port: # Only add port to config if it's valid\n",
    "        config['port'] = db_port\n",
    "    \n",
    "    # Check for missing essential config values (excluding password, which PGPASSWORD or .pgpass might handle)\n",
    "    for key, value in config.items():\n",
    "        if value is None and key not in ['port', 'password']: \n",
    "             print(f\"Warning: Environment variable for 'DB_{key.upper()}' is not set. Connection might fail.\")\n",
    "             \n",
    "    return config\n",
    "\n",
    "def export_single_table_to_csv(table_name: str, output_dir: str, db_config: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Export a single specified table to a CSV file with a timestamp in the filename.\n",
    "    Returns True on success, False on failure.\n",
    "    Uses the provided db_config.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    \n",
    "    # Validate db_config essentials for connection\n",
    "    required_keys_for_connection = ['dbname', 'user'] # host, port, password can be optional depending on setup\n",
    "    for key in required_keys_for_connection:\n",
    "        if not db_config.get(key):\n",
    "            print(f\"Error: Database configuration for '{key}' (DB_{key.upper()}) is missing. Cannot connect for table '{table_name}'.\")\n",
    "            return False\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get column names safely\n",
    "        # Note: table_name here is from a predefined list, so direct use in string is less risky,\n",
    "        # but using parameters for information_schema queries is still good practice.\n",
    "        cursor.execute(\n",
    "            sql.SQL(\"\"\"\n",
    "                SELECT column_name \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_schema = 'public' \n",
    "                AND table_name = %s\n",
    "                ORDER BY ordinal_position;\n",
    "            \"\"\"), (table_name,)\n",
    "        )\n",
    "        \n",
    "        columns_result = cursor.fetchall()\n",
    "        if not columns_result:\n",
    "            print(f\"Warning: Table '{table_name}' not found in schema 'public', or it has no columns. Skipping.\")\n",
    "            return True # Considered \"successful\" for batch operation as table might not exist\n",
    "        columns = [col[0] for col in columns_result]\n",
    "        \n",
    "        # Create filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # output_dir is already created by the calling function\n",
    "        filename = os.path.join(output_dir, f'{table_name}_export_{timestamp}.csv')\n",
    "        \n",
    "        # Construct the SELECT query for data using psycopg2.sql.Identifier for safety\n",
    "        query_data = sql.SQL(\"SELECT * FROM public.{}\").format(sql.Identifier(table_name))\n",
    "        cursor.execute(query_data)\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns) # Write header\n",
    "            for row_data in cursor:\n",
    "                writer.writerow(row_data) # Write data\n",
    "        \n",
    "        print(f\"Table '{table_name}' successfully exported to {filename}\")\n",
    "        return True\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error for table '{table_name}': {e}\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"File system error for table '{table_name}' (e.g., cannot write file): {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while exporting table '{table_name}': {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if conn: conn.close()\n",
    "\n",
    "# --- Main Script for Exporting the Predefined Set of Tables ---\n",
    "\n",
    "TABLES_TO_EXPORT_PREDEFINED = [\n",
    "    \"user\", \"general_settings\", \"Subscription\", \"FailedPayment\",\n",
    "    \"paid_user_info\", \"PaymentDetails\", \"car_info\", \"insurance_details\",\n",
    "    \"registration_details\", \"request_maintanainance\", \"car_expense\",\n",
    "    \"inspection_report\", \"reminder\", \"entry_inspection\", \"task\",\n",
    "    \"maintanainance_history\", \"maintenance_alert\", \"vehicle_document_alerts\",\n",
    "    \"connected_car\", \"brand\", \"location\", \"location_city\", \"designation\",\n",
    "    \"employee\"\n",
    "]\n",
    "\n",
    "def export_all_predefined_tables():\n",
    "    \"\"\"Exports all tables listed in TABLES_TO_EXPORT_PREDEFINED to individual CSV files.\"\"\"\n",
    "    \n",
    "    print(f\"Starting export for {len(TABLES_TO_EXPORT_PREDEFINED)} predefined tables...\")\n",
    "    \n",
    "    # Load DB config once for the entire batch\n",
    "    db_config = load_db_config()\n",
    "    if not db_config.get('dbname'): # Critical check from load_db_config (though it prints a warning)\n",
    "        print(\"Error: DB_NAME is not configured. Cannot proceed with export.\")\n",
    "        return\n",
    "\n",
    "    # Create a unique parent directory for this export batch\n",
    "    timestamp_dir_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # Use db_config['dbname'] safely, ensuring it's not None\n",
    "    db_name_for_folder = db_config.get('dbname', 'unknown_db')\n",
    "    base_export_dir_name = f\"{db_name_for_folder}_predefined_set_export_{timestamp_dir_str}\"\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(base_export_dir_name, exist_ok=True)\n",
    "        print(f\"All CSVs will be saved in directory: '{base_export_dir_name}'\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create export directory '{base_export_dir_name}': {e}. Aborting export.\")\n",
    "        return\n",
    "\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    for table_name in TABLES_TO_EXPORT_PREDEFINED:\n",
    "        print(f\"\\nProcessing table: {table_name}...\")\n",
    "        if export_single_table_to_csv(table_name, output_dir=base_export_dir_name, db_config=db_config):\n",
    "            success_count += 1\n",
    "        else:\n",
    "            fail_count += 1\n",
    "            print(f\"Failed to export table: {table_name}\")\n",
    "    \n",
    "    print(f\"\\n--- Export Summary for Predefined Set ---\")\n",
    "    print(f\"Output Directory: {base_export_dir_name}\")\n",
    "    print(f\"Total tables attempted: {len(TABLES_TO_EXPORT_PREDEFINED)}\")\n",
    "    print(f\"Successfully exported: {success_count}\")\n",
    "    print(f\"Failed to export: {fail_count}\")\n",
    "    \n",
    "    if fail_count == 0 and success_count > 0:\n",
    "        print(\"All tables from the predefined set were exported successfully.\")\n",
    "    elif success_count == 0 and fail_count == 0 and TABLES_TO_EXPORT_PREDEFINED: # Should not occur if list not empty\n",
    "        print(\"No tables were processed (list might be empty or all tables skipped).\")\n",
    "    elif not TABLES_TO_EXPORT_PREDEFINED:\n",
    "         print(\"The list of tables to export was empty.\")\n",
    "    else:\n",
    "        print(\"Some tables from the predefined set failed to export. Please review the logs above for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting export for 24 predefined tables...\n",
      "All CSVs will be saved in directory: 'fleetblox-dev_predefined_set_export_20250521_201126'\n",
      "\n",
      "Processing table: user...\n",
      "Table 'user' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\user_export_20250521_201127.csv\n",
      "\n",
      "Processing table: general_settings...\n",
      "Table 'general_settings' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\general_settings_export_20250521_201127.csv\n",
      "\n",
      "Processing table: Subscription...\n",
      "Table 'Subscription' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\Subscription_export_20250521_201128.csv\n",
      "\n",
      "Processing table: FailedPayment...\n",
      "Table 'FailedPayment' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\FailedPayment_export_20250521_201129.csv\n",
      "\n",
      "Processing table: paid_user_info...\n",
      "Table 'paid_user_info' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\paid_user_info_export_20250521_201130.csv\n",
      "\n",
      "Processing table: PaymentDetails...\n",
      "Table 'PaymentDetails' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\PaymentDetails_export_20250521_201130.csv\n",
      "\n",
      "Processing table: car_info...\n",
      "Table 'car_info' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\car_info_export_20250521_201131.csv\n",
      "\n",
      "Processing table: insurance_details...\n",
      "Table 'insurance_details' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\insurance_details_export_20250521_201132.csv\n",
      "\n",
      "Processing table: registration_details...\n",
      "Table 'registration_details' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\registration_details_export_20250521_201133.csv\n",
      "\n",
      "Processing table: request_maintanainance...\n",
      "Table 'request_maintanainance' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\request_maintanainance_export_20250521_201134.csv\n",
      "\n",
      "Processing table: car_expense...\n",
      "Table 'car_expense' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\car_expense_export_20250521_201134.csv\n",
      "\n",
      "Processing table: inspection_report...\n",
      "Table 'inspection_report' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\inspection_report_export_20250521_201135.csv\n",
      "\n",
      "Processing table: reminder...\n",
      "Table 'reminder' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\reminder_export_20250521_201136.csv\n",
      "\n",
      "Processing table: entry_inspection...\n",
      "Table 'entry_inspection' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\entry_inspection_export_20250521_201136.csv\n",
      "\n",
      "Processing table: task...\n",
      "Table 'task' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\task_export_20250521_201137.csv\n",
      "\n",
      "Processing table: maintanainance_history...\n",
      "Table 'maintanainance_history' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\maintanainance_history_export_20250521_201138.csv\n",
      "\n",
      "Processing table: maintenance_alert...\n",
      "Table 'maintenance_alert' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\maintenance_alert_export_20250521_201138.csv\n",
      "\n",
      "Processing table: vehicle_document_alerts...\n",
      "Table 'vehicle_document_alerts' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\vehicle_document_alerts_export_20250521_201139.csv\n",
      "\n",
      "Processing table: connected_car...\n",
      "Table 'connected_car' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\connected_car_export_20250521_201140.csv\n",
      "\n",
      "Processing table: brand...\n",
      "Table 'brand' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\brand_export_20250521_201141.csv\n",
      "\n",
      "Processing table: location...\n",
      "Table 'location' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\location_export_20250521_201141.csv\n",
      "\n",
      "Processing table: location_city...\n",
      "Table 'location_city' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\location_city_export_20250521_201142.csv\n",
      "\n",
      "Processing table: designation...\n",
      "Table 'designation' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\designation_export_20250521_201143.csv\n",
      "\n",
      "Processing table: employee...\n",
      "Table 'employee' successfully exported to fleetblox-dev_predefined_set_export_20250521_201126\\employee_export_20250521_201143.csv\n",
      "\n",
      "--- Export Summary for Predefined Set ---\n",
      "Output Directory: fleetblox-dev_predefined_set_export_20250521_201126\n",
      "Total tables attempted: 24\n",
      "Successfully exported: 24\n",
      "Failed to export: 0\n",
      "All tables from the predefined set were exported successfully.\n",
      "\n",
      "Script execution finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Execute the export ---\n",
    "# This will run when you execute the cell in Jupyter Notebook.\n",
    "# Ensure your .env file is set up correctly.\n",
    "\n",
    "# Call the main function to start the export process\n",
    "export_all_predefined_tables()\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Database Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an export method:\n",
      "1. Full database backup using pg_dump (recommended for backups)\n",
      "2. Export all tables to individual CSV files (data only)\n",
      "3. Export a single table to CSV\n",
      "Exporting all tables to directory: ninja-dev_all_tables_csv_20250517_230044\n",
      "\n",
      "Exporting table: _prisma_migrations...\n",
      "Table '_prisma_migrations' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\_prisma_migrations_export_20250517_230045.csv\n",
      "\n",
      "Exporting table: task...\n",
      "Table 'task' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\task_export_20250517_230045.csv\n",
      "\n",
      "Exporting table: otp...\n",
      "Table 'otp' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\otp_export_20250517_230046.csv\n",
      "\n",
      "Exporting table: purchase_plan...\n",
      "Table 'purchase_plan' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\purchase_plan_export_20250517_230046.csv\n",
      "\n",
      "Exporting table: newLetter...\n",
      "Table 'newLetter' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\newLetter_export_20250517_230047.csv\n",
      "\n",
      "Exporting table: interestUser...\n",
      "Table 'interestUser' successfully exported to ninja-dev_all_tables_csv_20250517_230044\\interestUser_export_20250517_230047.csv\n",
      "\n",
      "--- Export Summary ---\n",
      "Successfully exported tables: 6\n",
      "Failed to export tables: 0\n",
      "All tables exported successfully to CSVs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_db_config() -> dict:\n",
    "    \"\"\"Load database configuration from environment variables.\"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    db_port_str = os.getenv('DB_PORT')\n",
    "    db_port = None\n",
    "    if db_port_str and db_port_str.isdigit():\n",
    "        db_port = int(db_port_str)\n",
    "    elif db_port_str:\n",
    "        print(f\"Warning: DB_PORT ('{db_port_str}') is not a valid number. Using default port if applicable.\")\n",
    "\n",
    "    config = {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USERNAME'),\n",
    "        'password': os.getenv('DB_PASS'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "    }\n",
    "    if db_port:\n",
    "        config['port'] = db_port\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        if value is None and key not in ['port', 'password']: # Password can be handled by PGPASSWORD or .pgpass\n",
    "             print(f\"Warning: Environment variable for '{key.upper()}' (e.g., DB_{key.upper()}) is not set.\")\n",
    "             \n",
    "    return config\n",
    "\n",
    "def export_entire_database_pg_dump(output_format: str = 'custom'):\n",
    "    \"\"\"\n",
    "    Export the entire database using pg_dump.\n",
    "    Requires pg_dump to be in the system's PATH or provide full path.\n",
    "    \n",
    "    Args:\n",
    "        output_format (str): 'custom' (for .dump, compressed, pg_restore), \n",
    "                             'plain' (for .sql, text SQL commands),\n",
    "                             'directory' (for parallel dumps).\n",
    "                             Defaults to 'custom'.\n",
    "    \"\"\"\n",
    "    db_config = load_db_config()\n",
    "\n",
    "    if not db_config.get('dbname'):\n",
    "        print(\"Error: DB_NAME is not configured. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Determine file extension and pg_dump format option\n",
    "    if output_format == 'custom':\n",
    "        file_extension = 'dump'\n",
    "        format_option = '-Fc' # Custom format (compressed, good for pg_restore)\n",
    "    elif output_format == 'plain':\n",
    "        file_extension = 'sql'\n",
    "        format_option = '-Fp' # Plain SQL text\n",
    "    elif output_format == 'directory':\n",
    "        # For directory format, pg_dump expects a directory name, not a file\n",
    "        # We'll create a directory and pg_dump will populate it.\n",
    "        # The filename variable will represent the directory name.\n",
    "        file_extension = '' # No extension for directory name\n",
    "        format_option = '-Fd' # Directory format\n",
    "        output_dir_name = f\"{db_config['dbname']}_fulldb_export_{timestamp}\"\n",
    "        filename = output_dir_name # This is now a directory\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Error: Output directory '{filename}' already exists.\")\n",
    "            return\n",
    "        # No need to create the directory beforehand for -Fd, pg_dump does it.\n",
    "    else:\n",
    "        print(f\"Error: Unsupported output format '{output_format}'. Choose 'custom', 'plain', or 'directory'.\")\n",
    "        return\n",
    "\n",
    "    if output_format != 'directory':\n",
    "        filename = f\"{db_config['dbname']}_fulldb_export_{timestamp}.{file_extension}\"\n",
    "\n",
    "\n",
    "    pg_dump_command = ['pg_dump']\n",
    "    \n",
    "    # Add connection parameters\n",
    "    if db_config.get('host'):\n",
    "        pg_dump_command.extend(['-h', db_config['host']])\n",
    "    if db_config.get('port'):\n",
    "        pg_dump_command.extend(['-p', str(db_config['port'])])\n",
    "    if db_config.get('user'):\n",
    "        pg_dump_command.extend(['-U', db_config['user']])\n",
    "    \n",
    "    pg_dump_command.append(format_option)\n",
    "    pg_dump_command.append(db_config['dbname'])\n",
    "\n",
    "    # Environment variables for pg_dump\n",
    "    env = os.environ.copy()\n",
    "    if db_config.get('password'):\n",
    "        env['PGPASSWORD'] = db_config['password']\n",
    "        # Note: It's generally more secure to use a .pgpass file for pg_dump\n",
    "        # or rely on other authentication methods (like peer authentication if applicable).\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting database dump for '{db_config['dbname']}'...\")\n",
    "        print(f\"Command (password hidden): {' '.join(pg_dump_command)} > {filename if output_format != 'directory' else output_dir_name}\")\n",
    "\n",
    "        if output_format == 'directory':\n",
    "            pg_dump_command.extend(['-f', filename]) # -f specifies output directory for -Fd\n",
    "            process = subprocess.Popen(pg_dump_command, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout, stderr = process.communicate()\n",
    "        else: # custom or plain\n",
    "            with open(filename, 'wb') as f_out: # 'wb' for binary formats like custom\n",
    "                process = subprocess.Popen(pg_dump_command, stdout=f_out, stderr=subprocess.PIPE, env=env)\n",
    "                stdout, stderr = process.communicate() # stdout is redirected to file, capture stderr\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            print(f\"Database '{db_config['dbname']}' successfully exported to '{filename}'\")\n",
    "            if stderr:\n",
    "                print(f\"pg_dump warnings/messages:\\n{stderr.decode()}\")\n",
    "        else:\n",
    "            print(f\"Error during pg_dump (return code: {process.returncode}):\")\n",
    "            print(f\"STDOUT:\\n{stdout.decode() if stdout else 'N/A'}\")\n",
    "            print(f\"STDERR:\\n{stderr.decode() if stderr else 'N/A'}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'pg_dump' command not found. Make sure PostgreSQL client tools are installed and in your PATH.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- The single table export function from your previous code (can be kept for individual table exports) ---\n",
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "def export_table_to_csv(table_name: str, output_dir: str = \".\"):\n",
    "    \"\"\"Export a single specified table to a CSV file with timestamp in filename.\"\"\"\n",
    "    # (This is the function you provided earlier, slightly adapted for output directory)\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        db_config = load_db_config()\n",
    "        if not db_config.get('dbname') or not db_config.get('user'):\n",
    "            print(\"Error: Database name or user not configured. Please check your .env file.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name = %s\n",
    "            ORDER BY ordinal_position;\n",
    "        \"\"\", (table_name,))\n",
    "        \n",
    "        columns_result = cursor.fetchall()\n",
    "        if not columns_result:\n",
    "            print(f\"Warning: Table '{table_name}' not found in schema 'public', or it has no columns. Skipping.\")\n",
    "            return True # Not an error, just skipping\n",
    "        columns = [col[0] for col in columns_result]\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        # Ensure output_dir exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = os.path.join(output_dir, f'{table_name}_export_{timestamp}.csv')\n",
    "        \n",
    "        query_data = sql.SQL(\"SELECT * FROM public.{}\").format(sql.Identifier(table_name))\n",
    "        cursor.execute(query_data)\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            for row in cursor:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Table '{table_name}' successfully exported to {filename}\")\n",
    "        return True # Indicate success\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error for table {table_name}: {e}\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"File system error for table {table_name} (e.g., cannot write file): {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for table {table_name}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if cursor: cursor.close()\n",
    "        if conn: conn.close()\n",
    "\n",
    "# Method 2: Python script to export all tables to individual CSV files (Data Only, No Schema)\n",
    "# This is NOT a full database backup, but can be useful if you just need data in CSVs.\n",
    "\n",
    "def export_all_tables_to_csvs():\n",
    "    \"\"\"Exports all user tables from the 'public' schema to individual CSV files.\"\"\"\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    db_config = load_db_config()\n",
    "    if not db_config.get('dbname'):\n",
    "        print(\"Error: DB_NAME is not configured. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get all table names in the 'public' schema\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT tablename \n",
    "            FROM pg_catalog.pg_tables \n",
    "            WHERE schemaname = 'public';\n",
    "        \"\"\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        if not tables:\n",
    "            print(\"No tables found in the 'public' schema.\")\n",
    "            return\n",
    "\n",
    "        # Create a directory for this export batch\n",
    "        timestamp_dir = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        base_export_dir = f\"{db_config['dbname']}_all_tables_csv_{timestamp_dir}\"\n",
    "        os.makedirs(base_export_dir, exist_ok=True)\n",
    "        print(f\"Exporting all tables to directory: {base_export_dir}\")\n",
    "\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        for table_name in tables:\n",
    "            print(f\"\\nExporting table: {table_name}...\")\n",
    "            if export_table_to_csv(table_name, output_dir=base_export_dir):\n",
    "                success_count += 1\n",
    "            else:\n",
    "                fail_count += 1\n",
    "        \n",
    "        print(f\"\\n--- Export Summary ---\")\n",
    "        print(f\"Successfully exported tables: {success_count}\")\n",
    "        print(f\"Failed to export tables: {fail_count}\")\n",
    "        if fail_count == 0:\n",
    "            print(\"All tables exported successfully to CSVs.\")\n",
    "        else:\n",
    "            print(\"Some tables failed to export. Check logs above.\")\n",
    "\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database connection or initial query error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose an export method:\")\n",
    "    print(\"1. Full database backup using pg_dump (recommended for backups)\")\n",
    "    print(\"2. Export all tables to individual CSV files (data only)\")\n",
    "    print(\"3. Export a single table to CSV\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1, 2, or 3): \").strip()\n",
    "\n",
    "    if choice == '1':\n",
    "        print(\"\\nChoose pg_dump output format:\")\n",
    "        print(\"  c. Custom (.dump - compressed, for pg_restore)\")\n",
    "        print(\"  p. Plain SQL (.sql - text SQL commands)\")\n",
    "        print(\"  d. Directory (outputs to a directory, good for parallel)\")\n",
    "        format_choice_input = input(\"Enter format (c, p, d) [default: c]: \").strip().lower()\n",
    "        \n",
    "        dump_format = 'custom' # default\n",
    "        if format_choice_input == 'p':\n",
    "            dump_format = 'plain'\n",
    "        elif format_choice_input == 'd':\n",
    "            dump_format = 'directory'\n",
    "        elif format_choice_input == 'c' or not format_choice_input:\n",
    "            dump_format = 'custom' # explicit default\n",
    "        else:\n",
    "            print(\"Invalid format choice. Using 'custom'.\")\n",
    "\n",
    "        export_entire_database_pg_dump(output_format=dump_format)\n",
    "    elif choice == '2':\n",
    "        export_all_tables_to_csvs()\n",
    "    elif choice == '3':\n",
    "        target_table = input(\"Enter the name of the single table to export: \").strip()\n",
    "        if not target_table:\n",
    "            print(\"No table name provided. Exiting.\")\n",
    "        else:\n",
    "            if not target_table.replace('_', '').isalnum(): # Basic check\n",
    "                print(f\"Warning: Table name '{target_table}' may contain invalid characters.\")\n",
    "            export_table_to_csv(target_table) # Uses current dir by default\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
